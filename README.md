# Call Shadow AI Agent - Brique LangChain

Copilote intelligent en temps rÃ©el pour conversations, basÃ© sur LangChain. Analyse les conversations live, dÃ©tecte les signaux clÃ©s, et propose des suggestions intelligentes pour guider l'utilisateur vers ses objectifs.

## ğŸ¯ FonctionnalitÃ©s

- **Analyse en temps rÃ©el** : Traite les conversations au fil de l'eau via WebSocket ou REST
- **MÃ©moire contextuelle** : Maintient l'historique complet avec mÃ©tadonnÃ©es (sentiment, Ã©motion, speaker)
- **Agent orchestrateur** : Utilise GPT-4o mini via LangChain pour gÃ©nÃ©rer des suggestions intelligentes
- **DÃ©tection de signaux** : Identifie objections, hÃ©sitations, intÃ©rÃªts, opportunitÃ©s
- **Suggestions tactiques** : Propose des questions et directions stratÃ©giques
- **Architecture modulaire** : Facile d'ajouter agents, tools, sources de donnÃ©es

## ğŸ“‹ PrÃ©requis

- Python 3.10+
- OpenAI API Key
- (Optionnel) Weaviate pour RAG

## ğŸš€ Installation

### 1. Cloner et installer les dÃ©pendances

```bash
cd /home/ricardo/projects/lngc-service
pip install -r requirements.txt
```

### 2. Configurer les variables d'environnement

CrÃ©er un fichier `.env` Ã  la racine du projet (utiliser `.env.example` comme template) :

```bash
# OpenAI Configuration
OPENAI_API_KEY=sk-...votre_clÃ©_ici
OPENAI_MODEL=gpt-4o-mini
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=500

# Application Configuration
APP_NAME=Call Shadow AI Agent
APP_VERSION=1.0.0
DEBUG=True
LOG_LEVEL=INFO

# API Configuration
HOST=0.0.0.0
PORT=8000
CORS_ORIGINS=["*"]

# Memory Configuration
MAX_MEMORY_MESSAGES=50
MEMORY_SUMMARY_ENABLED=False
```

### 3. Lancer le service

```bash
# Depuis la racine du projet
python -m app.main

# Ou avec uvicorn directement
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

Le service sera accessible sur :
- **WebSocket** : `ws://localhost:8000/ws/conversation`
- **REST API** : `http://localhost:8000/api/process`
- **Documentation** : `http://localhost:8000/docs`

## ğŸ“¡ Utilisation

### Via WebSocket (recommandÃ© pour temps rÃ©el)

```python
import asyncio
import websockets
import json

async def test_websocket():
    uri = "ws://localhost:8000/ws/conversation"
    
    async with websockets.connect(uri) as websocket:
        # Envoyer un message
        message = {
            "text": "Yes, I'm interested but I'm not sure about the pricing.",
            "speaker": "client",
            "sentiment": "negative",
            "emotion": "uncertain"
        }
        
        await websocket.send(json.dumps(message))
        
        # Recevoir la suggestion
        response = await websocket.recv()
        suggestion = json.loads(response)
        
        print("Questions suggÃ©rÃ©es:", suggestion["questions"])
        print("Signaux dÃ©tectÃ©s:", suggestion["signals_detected"])
        print("Direction:", suggestion["recommended_direction"])

asyncio.run(test_websocket())
```

Ou utiliser le script de test fourni :

```bash
python test_client.py
```

### Via REST API

```bash
curl -X POST "http://localhost:8000/api/process" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "I am not sure about the pricing",
    "speaker": "client",
    "sentiment": "negative",
    "emotion": "uncertain"
  }'
```

RÃ©ponse :

```json
{
  "questions": [
    "Would you like more details about the pricing structure?",
    "What aspects of the pricing concern you most?"
  ],
  "signals_detected": [
    "uncertainty about pricing",
    "hesitation",
    "potential objection"
  ],
  "recommended_direction": "Clarify pricing model and emphasize value proposition."
}
```

## ğŸ—ï¸ Architecture

```
app/
â”œâ”€â”€ main.py                          # Point d'entrÃ©e FastAPI + WebSocket
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py                  # Configuration centralisÃ©e (.env)
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ input.py                     # InputMessage (Pydantic)
â”‚   â””â”€â”€ output.py                    # OutputSuggestion (Pydantic)
â”œâ”€â”€ memory/
â”‚   â””â”€â”€ conversation_memory.py       # MÃ©moire conversationnelle custom
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ orchestrator.py              # Agent principal (LCEL)
â”œâ”€â”€ tools/
â”‚   â””â”€â”€ weaviate_tool.py             # Tool RAG (prÃ©parÃ© pour plus tard)
â”œâ”€â”€ handlers/
â”‚   â””â”€â”€ stream_handler.py            # Pipeline de traitement streaming
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ websocket.py                 # WebSocket endpoint
â”‚   â””â”€â”€ rest.py                      # REST endpoints
â””â”€â”€ utils/
    â””â”€â”€ logger.py                    # Configuration logs
```

### Flux de traitement

1. **RÃ©ception** : Message JSON reÃ§u via WebSocket ou REST
2. **Validation** : Pydantic valide le format InputMessage
3. **MÃ©moire** : Message ajoutÃ© Ã  l'historique contextuel
4. **Analyse** : Agent orchestrateur analyse avec GPT-4o mini
5. **GÃ©nÃ©ration** : Suggestions structurÃ©es (OutputSuggestion)
6. **RÃ©ponse** : JSON renvoyÃ© au client

### LCEL (LangChain Expression Language)

L'agent orchestrateur utilise LCEL pour composer le pipeline :

```python
chain = (
    RunnableLambda(prepare_inputs)  # Enrichit avec contexte mÃ©moire
    | prompt                         # Template de prompt
    | llm                            # GPT-4o mini
    | output_parser                  # Parse JSON â†’ OutputSuggestion
)
```

## ğŸ“š Endpoints disponibles

### WebSocket

- `ws://localhost:8000/ws/conversation` - Connexion temps rÃ©el
- `GET /ws/status` - Statut du handler WebSocket
- `POST /ws/clear` - Efface la mÃ©moire

### REST API

- `POST /api/process` - Traite un message unique
- `GET /api/conversation/context` - RÃ©cupÃ¨re le contexte complet
- `GET /api/conversation/summary` - Statistiques de la conversation
- `POST /api/conversation/clear` - Efface la mÃ©moire

### Utilitaires

- `GET /` - Informations de base
- `GET /health` - Health check
- `GET /docs` - Documentation Swagger auto-gÃ©nÃ©rÃ©e

## ğŸ”§ Extension et Personnalisation

### Ajouter un nouvel agent

1. CrÃ©er `app/agents/mon_agent.py`
2. ImplÃ©menter la logique avec LCEL
3. Importer et utiliser dans `orchestrator.py` ou `stream_handler.py`

### Ajouter un tool

1. CrÃ©er `app/tools/mon_tool.py`
2. DÃ©corer la fonction avec `@tool`
3. Binder au LLM dans `orchestrator.py` :

```python
from app.tools.mon_tool import mon_tool_function

llm_with_tools = llm.bind_tools([mon_tool_function])
```

### Changer de modÃ¨le LLM

Modifier dans `.env` :

```bash
OPENAI_MODEL=gpt-4o        # ou gpt-4, gpt-3.5-turbo, etc.
```

### Activer Weaviate (RAG)

1. Configurer dans `.env` :

```bash
WEAVIATE_URL=https://your-instance.weaviate.network
WEAVIATE_API_KEY=your_api_key_here
WEAVIATE_CLASS=ConversationKnowledge
```

2. DÃ©commenter le code dans `app/tools/weaviate_tool.py`

3. Binder le tool dans `orchestrator.py` :

```python
from app.tools.weaviate_tool import weaviate_search

llm_with_tools = llm.bind_tools([weaviate_search])
```

## ğŸ§ª Tests

### Script de test WebSocket

```bash
python test_client.py
```

### Tests manuels

```bash
# Health check
curl http://localhost:8000/health

# Statut WebSocket
curl http://localhost:8000/ws/status

# RÃ©sumÃ© conversation
curl http://localhost:8000/api/conversation/summary
```

## ğŸ¨ Cas d'usage

- **Ventes** : DÃ©tecte objections, propose relances
- **Support** : Identifie frustrations, suggÃ¨re solutions
- **Interviews** : Guide questions de discovery
- **NÃ©gociations** : Analyse sentiment, recommande tactiques
- **Formation** : Coach en temps rÃ©el pour commerciaux

## ğŸ“Š Format des donnÃ©es

### InputMessage

```json
{
  "text": "Texte transcrit du message",
  "speaker": "client" | "agent",
  "sentiment": "positive" | "negative" | "neutral",
  "emotion": "joy" | "anger" | "uncertain" | "neutral"
}
```

### OutputSuggestion

```json
{
  "questions": ["Question 1", "Question 2"],
  "signals_detected": ["Signal 1", "Signal 2"],
  "recommended_direction": "Direction stratÃ©gique claire"
}
```

## ğŸ”’ SÃ©curitÃ©

- **API Keys** : Ne jamais commiter le fichier `.env`
- **CORS** : Configurer `CORS_ORIGINS` en production
- **Rate limiting** : ConsidÃ©rer l'ajout en production
- **Authentication** : Ajouter si nÃ©cessaire (JWT, OAuth)

## ğŸš¦ Production

Recommandations pour dÃ©ploiement :

1. **Sessions** : ImplÃ©menter gestion de sessions par utilisateur/appel
2. **Redis** : Externaliser la mÃ©moire dans Redis pour scalabilitÃ©
3. **Monitoring** : Ajouter Prometheus/Grafana pour mÃ©triques
4. **Load balancing** : Utiliser plusieurs instances derriÃ¨re un LB
5. **Secrets** : Utiliser un gestionnaire de secrets (AWS Secrets Manager, Vault)

## ğŸ“ Logs

Logs colorÃ©s avec niveaux :
- **DEBUG** : DÃ©tails de traitement
- **INFO** : Ã‰vÃ©nements importants
- **WARNING** : ProblÃ¨mes non critiques
- **ERROR** : Erreurs avec stack trace

Configurer le niveau dans `.env` :

```bash
LOG_LEVEL=DEBUG  # ou INFO, WARNING, ERROR
```

## ğŸ—ºï¸ Roadmap

- [ ] Multi-agents orchestration (stratÃ©gie, tactique, technique)
- [ ] Streaming des suggestions (token par token)
- [ ] IntÃ©gration Weaviate complÃ¨te (RAG)
- [ ] Call blueprint dynamique
- [ ] Analyse post-call (rÃ©sumÃ©, insights, actions)
- [ ] Templates de prompts par use case (sales, support, etc.)
- [ ] MÃ©triques et analytics dashboard
- [ ] Support d'autres LLMs (Claude, Mistral, local)

## ğŸ¤ Contribution

Architecture pensÃ©e pour Ãªtre extensible. Pour contribuer :

1. Fork le projet
2. CrÃ©er une branche feature
3. Commiter les changements
4. Pousser et crÃ©er une PR

## ğŸ“„ Licence

Projet privÃ© - Tous droits rÃ©servÃ©s

## ğŸ“§ Support

Pour questions ou support, contacter l'Ã©quipe de dÃ©veloppement.

---

**Call Shadow AI Agent** - Votre copilote intelligent pour conversations en temps rÃ©el ğŸš€

# lngc-service
